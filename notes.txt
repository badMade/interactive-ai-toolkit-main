Purpose of the Program
----------------------
- Provide accessible speech tools by combining Whisper-based speech-to-text and Hugging Face SpeechT5 text-to-speech utilities.
- Help educators and learners rapidly prototype inclusive lessons without depending on proprietary cloud platforms.
- Demonstrate both offline workflows and optional hosted API integrations for transcription and synthesis.

Prerequisites
-------------
- Python 3.10 or newer (virtual environments recommended for isolation).
- FFmpeg installed and available on PATH so Whisper can decode audio (verify with ``ffmpeg -version``).
- Internet access the first time the Whisper and SpeechT5 model weights download from PyPI and Hugging Face.
- Optional: ``openai`` Python client when you plan to call the hosted Whisper API.

Why Some Files Are Not Tracked
------------------------------
- ``lesson_recording.mp3`` is intentionally excluded to keep the repository lightweight; supply your own lesson audio when following the walkthrough.
- Hugging Face caches (for SpeechT5) live under ``~/.cache/huggingface`` and are not checked into source control.

Hands-on Walkthrough
--------------------
1. Create and activate a virtual environment.
   - Windows (PowerShell): ``py -3.12 -m venv .venv`` then ``.\.venv\Scripts\Activate``
   - macOS/Linux (bash or zsh): ``python3 -m venv .venv`` then ``source .venv/bin/activate``
2. Upgrade packaging tooling and install dependencies: ``python -m pip install --upgrade pip`` followed by ``python -m pip install -r requirements.txt``.
3. Install FFmpeg (Windows builds from Gyan.dev, ``brew install ffmpeg`` on macOS, or ``sudo apt install ffmpeg`` on Debian/Ubuntu) and confirm the command works.
4. Copy or record an audio sample (for example ``lesson_recording.mp3``) into the project root.
5. Run offline transcription: ``python transcribe.py [audio_path] [--model MODEL_NAME]`` and review the printed transcript.
6. Generate speech from text: ``python tts.py`` to create ``output.wav`` using the deterministic SpeechT5 voice.
7. Repeat or adapt the scripts inside other Python programs to automate inclusive lesson preparation.

Optional API Integration
------------------------
1. Install the OpenAI client if needed: ``python -m pip install openai``.
2. Configure ``OPENAI_API_KEY`` for repeatable use.
   - Windows (PowerShell): ``setx OPENAI_API_KEY "sk-..."`` then start a new shell; for the current session use ``$env:OPENAI_API_KEY = "sk-..."``.
   - macOS/Linux (bash or zsh): ``echo 'export OPENAI_API_KEY="sk-..."' >> ~/.bashrc`` then ``source ~/.bashrc``; to scope it to one session run ``export OPENAI_API_KEY="sk-..."``.
3. Follow the README example to call the hosted Whisper API with ``OpenAI().audio.transcriptions.create`` and print the transcript.
4. Use the hosted option when managed infrastructure or faster turnaround is preferable to local hardware.

Quick Setup Cheatsheet
----------------------
- ``python -m venv .venv`` â†’ activate it for your platform.
- ``python -m pip install --upgrade pip``
- ``python -m pip install -r requirements.txt``
- Ensure ``ffmpeg -version`` succeeds, then run ``python transcribe.py`` or ``python tts.py`` as needed.

Troubleshooting Tips
--------------------
- Whisper ``FileNotFoundError`` or FFmpeg issues: confirm FFmpeg is on PATH and that the audio file exists in the expected location.
- ``ImportError`` for ``sentencepiece`` or ``soundfile`` when running ``tts.py``: reinstall dependencies via ``python -m pip install -r requirements.txt``.
- PyTorch installation issues on Windows: install the CPU wheel ``python -m pip install torch --index-url https://download.pytorch.org/whl/cpu``.
- Slow performance on CPUs: choose smaller Whisper checkpoints (``tiny`` or ``small``) and keep demo text short for SpeechT5.
- Logs and debug output: inspect terminal logs; Hugging Face downloads are cached automatically under the user's home directory.
