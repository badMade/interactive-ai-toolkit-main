# Inclusive AI Toolkit

Inclusive AI Toolkit is a lightweight Python project that demonstrates how to
build accessible speech utilities with entirely local resources. It combines
OpenAI Whisper for speech-to-text (STT) and Hugging Face SpeechT5 for
text-to-speech (TTS) so educators can prototype inclusive learning experiences
without depending on proprietary cloud services.

## Features

- **Local transcription**: Run Whisper locally to transcribe lessons, meetings,
  or announcements without uploading audio to the cloud.
- **Speech synthesis**: Convert text into natural-sounding speech using the
  SpeechT5 model and a deterministic speaker embedding.
- **Extensible scripts**: Reusable helper functions make it easy to integrate
  transcription and synthesis into other Python applications.
- **Optional cloud integration**: The repository documents how to call the
  OpenAI Whisper API for scenarios where managed infrastructure is preferred.

## Quickstart

After installing the project dependencies you can call the Universal LLM CLI
directly. The interface supports plain prompts, streaming responses, tool
invocations, and embeddings while sharing configuration across commands.

```bash
python -m universal_llm.cli chat "Summarize today's lecture in plain language" \
  --model gpt-4o-mini --provider openai

# Use Claude 3.5 Sonnet for advanced reasoning tasks
python -m universal_llm.cli chat "Explain quantum computing to high school students" \
  --model claude-3-5-sonnet-20241022 --provider anthropic

# Stream responses in real time (``--json`` collects structured chunks)
python -m universal_llm.cli stream "Draft an inclusive classroom activity" \
  --model gpt-4o-mini --provider openai --json

# Load a JSON tool specification before invoking the model
python -m universal_llm.cli tools "Generate a differentiated lesson plan" \
  --tools path/to/tools.json --model gpt-4o-mini --provider openai

# Produce embeddings for later retrieval workflows
python -m universal_llm.cli embed "Universal design for learning" \
  --model text-embedding-3-small --provider openai
```

Use ``--config path/to/settings.yaml`` (JSON is also accepted) to centralise API
keys and model defaults. When a provider requires credentials like
``OPENAI_API_KEY`` and they are missing, the CLI exits with a helpful reminder to
set the appropriate environment variables or supply the configuration file, so
requests never fail silently.

## Repository Structure

```text
.
├── Images/                         # Optional screenshots for documentation
├── lesson_recording.mp3            # Sample audio tracked in version control for reproducible walkthroughs
├── notes.txt                       # Project summary generated by scripts/generate_notes.py
├── requirements.txt                # Python dependencies for Whisper and SpeechT5
├── scripts/
│   ├── ensure_requirements.py      # Validate Python dependency consistency
│   ├── generate_notes.py           # Helper to regenerate notes.txt
│   ├── install_ffmpeg.sh           # Bootstrap FFmpeg on supported platforms
│   ├── self_debug.py               # Interactive diagnostics for troubleshooting
│   └── verify_environment_setup.py # Sanity-check project prerequisites
├── transcribe.py                   # CLI for offline Whisper transcription
└── tts.py                          # CLI for SpeechT5 text-to-speech generation
```

## scripts/ Overview

- `scripts/ensure_requirements.py`: Validates Python dependency consistency against the lockfile expectations.
- `scripts/generate_notes.py`: Regenerates `notes.txt` from the latest transcription and synthesis outputs.
- `scripts/install_ffmpeg.sh`: Bootstraps FFmpeg on supported platforms for audio preprocessing.
- `scripts/self_debug.py`: Provides interactive diagnostics for troubleshooting local environments.
- `scripts/verify_environment_setup.py`: Performs sanity checks to confirm project prerequisites are satisfied.

## Universal LLM Client

The repository also ships with `universal_llm`, a provider-agnostic client that
routes chat, streaming, and embedding workloads across OpenAI, Azure OpenAI,
Anthropic (including Claude 3.5 Sonnet), Google Vertex AI, and locally hosted 
Ollama deployments. The facade exposes a consistent Pydantic-based data model 
and resilient HTTP helpers so applications can fail fast, retry transient errors, 
and swap providers without rewriting business logic.

### Installation

Install the package and its core dependencies directly from the repository
root:

```bash
python -m pip install .
```

Optional extras enable provider-specific helpers and YAML configuration
parsing:

```bash
python -m pip install .[providers]
python -m pip install .[openai]  # or .[anthropic], .[azure], .[vertex], .[ollama], .[yaml]
```

### Command Line Interface

The `python -m universal_llm.cli` entry point surfaces the same functionality as
the programmatic facade. Each command accepts ``--model`` and optionally
``--provider``/``--temperature`` so workflows stay consistent across providers.
Pair the CLI with ``--config`` to reuse credentials, rate limits, and default
models defined in JSON or YAML.

## Prerequisites

- Python **3.12**.
- [FFmpeg](https://ffmpeg.org/) for audio decoding when running Whisper.
- Internet access the first time you download the Whisper and SpeechT5 model
  weights from PyPI and Hugging Face.
- Optional: the `openai` Python client if you plan to call the hosted Whisper
  API.

For the best experience, use a virtual environment to isolate dependencies and
ensure the scripts work consistently across machines.

## Why Some Files Are Not Tracked

- `lesson_recording.mp3` intentionally remains in standard Git (not Git LFS) so
  the sample workflow has reproducible audio out of the box, with the
  `.gitignore` allow-list rule `!lesson_recording.mp3` documenting the decision.
- Hugging Face caches for SpeechT5 live under `~/.cache/huggingface` and do not
  belong in source control.

## Environment Setup

> **macOS fast recovery:** On macOS x86_64 you can run `./fix_env.sh` from the
> project root to recreate a fresh Python 3.12 virtual environment with the
> pinned dependencies listed in `requirements.txt`. The script is optional but
> provides a reliable baseline if manual setup falls out of sync.

### 1. Create and activate a virtual environment

#### Windows (PowerShell)

```powershell
py -3.12 -m venv .venv
.\.venv\Scripts\Activate
```

#### macOS / Linux (bash or zsh)

```bash
python3.12 -m venv .venv
source .venv/bin/activate
# Confirm the interpreter is the expected release
python3.12 --version
```

```bash
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

The requirements file pins the compatible toolchain (NumPy 1.26.4, PyTorch
2.2.2, Whisper 20250625, and related libraries) so you do not need to manage
individual package constraints manually.

> **Note:** When the CLI reports `OpenAI Whisper is not installed. Install it with 'pip install openai-whisper' or run setup_env.py to configure the environment.`, run `pip install openai-whisper==20250625` in your active environment to install the missing dependency. The version pin matches the latest stable release confirmed with `python -m pip index versions openai-whisper`.

### 3. Install FFmpeg

- **Windows**: Download a static build from
  [Gyan.dev](https://www.gyan.dev/ffmpeg/builds/) and either add `ffmpeg.exe` to
  your `PATH` or copy it into the repository root.
- **macOS**: `brew install ffmpeg`
- **Linux (Debian/Ubuntu)**: `sudo apt install ffmpeg`

Whisper depends on FFmpeg for audio preprocessing. When a native package or
system installer is unavailable, install the cross-platform fallback with
`python -m pip install "imageio[ffmpeg]"` to vendor a portable binary that
Whisper can use.

After installation, run `ffmpeg -version` to confirm the command is available on
your `PATH`.

### 4. (Optional) Install the OpenAI Python client

```bash
python -m pip install openai
```

The optional client is needed only if you plan to call the hosted Whisper API as
an alternative to local transcription.

### 5. Trusted certificates behind corporate proxies

If your network intercepts HTTPS traffic, install the proxy's root certificate
into your operating system trust store (Keychain Access on macOS, `certmgr.msc`
on Windows, or `/usr/local/share/ca-certificates` on Linux). Restart the shell
so Python uses the updated trust chain. When you cannot modify the global store,
save the root certificate in PEM format and run the tools with:

```bash
export REQUESTS_CA_BUNDLE=/path/to/proxy-root.pem
export CURL_CA_BUNDLE=/path/to/proxy-root.pem
export SSL_CERT_FILE=/path/to/proxy-root.pem
```

You can also pass the path directly to the CLI using `--ca-bundle` as shown
below. Avoid disabling TLS verification; trusting the certificate keeps model
downloads secure while resolving proxy errors.

## Usage

### Hands-on Walkthrough

1. Create and activate a virtual environment using the commands above.
2. Upgrade packaging tooling and install the pinned dependencies:

   ```bash
   python -m pip install --upgrade pip
   python -m pip install -r requirements.txt
   ```

3. Install FFmpeg (download a Windows build from Gyan.dev, run `brew install
   ffmpeg` on macOS, or `sudo apt install ffmpeg` on Debian/Ubuntu). After
   installation, run `ffmpeg -version` to confirm the command is available on
   your `PATH`.
4. Copy or record an audio sample (for example `lesson_recording.mp3`) into the
   project root.
5. Run offline transcription:

   ```bash
   python transcribe.py [audio_path] [--model MODEL_NAME]
   ```

   Review the printed transcript.
6. Generate speech from text:

   ```bash
   python tts.py
   ```

   This creates `output.wav` using the deterministic SpeechT5 voice.
7. Import the scripts into other Python programs when you need to automate
   inclusive lesson preparation.

### Local speech-to-text (`transcribe.py`)

`transcribe.py` wraps the Whisper model in a small command-line tool.

```bash
python transcribe.py [audio_path] [--model MODEL_NAME] [--fp16] [--ca-bundle PATH]
```

- `audio_path` (optional): Path to the audio file you want to transcribe. The
  default is `lesson_recording.mp3` in the project root.
- `--model`: Whisper checkpoint to load (e.g. `tiny`, `base`, `small`, `medium`,
  `large`). Larger models are slower but more accurate.
- `--fp16`: Request half-precision inference if your GPU/CPU supports it.

Example output:

```text
Transcript: Welcome to inclusive education with AI.
```

If the audio file cannot be found, the script raises a clear `FileNotFoundError`
so that automated callers can handle the failure.

### Text-to-speech (`tts.py`)

`tts.py` synthesizes speech using SpeechT5.

```bash
python tts.py
```

The script uses a deterministic speaker embedding to keep the generated voice
consistent between runs. By default it converts the string
`"Welcome to inclusive education with AI."` into `output.wav`. You can import
`main` from another module to generate custom speech programmatically:

```python
from tts import main as synthesize

synthesize(text="Custom narration", output_filename="narration.wav")
```

### Optional: Whisper via the OpenAI API

1. Create an API key at
   [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
2. Configure the `OPENAI_API_KEY` environment variable.

   - Windows (PowerShell):

     ```powershell
     setx OPENAI_API_KEY "sk-..."
     ```

     Restart the shell so the change takes effect. For the current session, you
     can instead run:

     ```powershell
     $env:OPENAI_API_KEY = "sk-..."
     ```

   - macOS / Linux (bash or zsh):

     ```bash
     echo 'export OPENAI_API_KEY="sk-..."' >> ~/.bashrc
     source ~/.bashrc
     ```

     To scope the key to a single terminal session, run:

     ```bash
     export OPENAI_API_KEY="sk-..."
     ```

3. Install the `openai` package if you have not already (`python -m pip install openai`).
4. Write a small helper similar to the example in this repository's history:

   ```python
   from openai import OpenAI

   client = OpenAI()
   with open("lesson_recording.mp3", "rb") as source:
       transcript = client.audio.transcriptions.create(
           model="whisper-1",
           file=source,
       )

   print(transcript.text)
   ```

Use the hosted option when you prefer managed infrastructure or need faster
results than local hardware can provide.

## Quick Setup Cheatsheet

- `python3.12 -m venv .venv` and activate it for your platform.
- `python -m pip install --upgrade pip`
- `python -m pip install "numpy<2"`
- `python -m pip install -r requirements.txt`
- After installing FFmpeg, run `ffmpeg -version` to confirm the command is on
  your `PATH`, then run `python transcribe.py` or `python tts.py`.

## Development Guidelines

- Follow the documented docstring style (Google-style descriptions of purpose,
  parameters, and return values) when extending the toolkit.
- Keep new functions pure where possible so they can be unit-tested without
  heavy mocking.
- Run `python -m compileall .` or your preferred test suite before committing to
  ensure syntax errors are caught early.
- Regenerate `notes.txt` via `python scripts/generate_notes.py` whenever
  instructions change so the shared documentation stays synchronized.

## Troubleshooting

| Symptom | Possible Cause | Suggested Fix |
| --- | --- | --- |
| `FileNotFoundError: Audio file not found` | Incorrect path supplied to `transcribe.py` | Provide the full audio path or copy the file into the repository root. |
| CLI exits with `OpenAI Whisper is not installed. Install it with 'pip install openai-whisper' or run setup_env.py to configure the environment.` | Whisper dependency missing from the environment | Install Whisper with `pip install openai-whisper` or run `python setup_env.py` to prepare the virtual environment. |
| `ffmpeg` errors | FFmpeg missing from `PATH` | Install FFmpeg and run `ffmpeg -version` to confirm the command is available on your `PATH`. |
| `ImportError: sentencepiece` or `soundfile` | Dependencies missing | Re-run `python -m pip install -r requirements.txt`. |
| Runtime errors mentioning NumPy 2.x wheels | Incompatible NumPy major release | Reinstall the compatible build with `python -m pip install "numpy<2"`. |
| PyTorch install fails on Windows | Default wheels conflict with CPU-only setups | Install the CPU build: `python -m pip install torch --index-url https://download.pytorch.org/whl/cpu`. |
| Generated audio sounds different between runs | Speaker embedding recreated randomly | Import and reuse `create_default_speaker_embedding()` or persist the tensor with `numpy.save`. |

## License

This project is released under the [MIT License](LICENSE).
